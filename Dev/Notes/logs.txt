                                                                                                                                                                            scala> val l2 = List()                                                                                                                                                      l2: List[Nothing] = List()                                                                                                                                                                                                                                                                                                                              scala> val l2 = List.fill(2)("eRR","errtyr")                                                                                                                                l2: List[(String, String)] = List((eRR,errtyr), (eRR,errtyr))                                                                                                                                                                                                                                                                                           scala> l2.foreach(println)                                                                                                                                                  (eRR,errtyr)                                                                                                                                                                (eRR,errtyr)                                                                                                                                                                                                                                                                                                                                            scala> val fruits =List("orange","mango","apple","strawberry","pineapple")                                                                                                  fruits: List[String] = List(orange, mango, apple, strawberry, pineapple)                                                                                                                                                                                                                                                                                scala> fruits.foreach(println)                                                                                                                                              orange                                                                                                                                                                      mango                                                                                                                                                                       apple                                                                                                                                                                       strawberry                                                                                                                                                                  pineapple                                                                                                                                                                                                                                                                                                                                               scala>                                                                                                                                                                      Display all 925 possibilities? (y or n)                                                                                                                                                                                                                                                                                                                 scala>                                                                                                                                                                                                                                                                                                                                                  scala> val l4 = "Java" :: "sql" :: "DBMS" :: "Pytorch"                                                                                                                      <console>:22: error: value :: is not a member of String                                                                                                                            val l4 = "Java" :: "sql" :: "DBMS" :: "Pytorch"                                                                                                                                                                ^                                                                                                                                                                                                                                                                                                             scala> val l4 = "Java" :: "sql" :: "DBMS" :: "Pytorch"::nil                                                                                                                 <console>:22: error: not found: value nil                                                                                                                                          val l4 = "Java" :: "sql" :: "DBMS" :: "Pytorch"::nil                                                                                                                                                                         ^                                                                                                                                                                                                                                                                                               scala> val l3 = "Java" :: "sql" :: "DBMS" :: "Pytorch"::nil                                                                                                                 <console>:22: error: not found: value nil                                                                                                                                          val l3 = "Java" :: "sql" :: "DBMS" :: "Pytorch"::nil                                                                                                                                                                         ^                                                                                                                                                                                                                                                                                               scala> val l3 = "Java" :: "sql" :: "DBMS" :: "Pytorch"                                                                                                                      <console>:22: error: value :: is not a member of String                                                                                                                            val l3 = "Java" :: "sql" :: "DBMS" :: "Pytorch"                                                                                                                                                                ^                                                                                                                                                                                                                                                                                                             scala> val l3 = "Java" :: "sql" :: "DBMS" :: "Pytorch":: nil                                                                                                                <console>:22: error: not found: value nil                                                                                                                                          val l3 = "Java" :: "sql" :: "DBMS" :: "Pytorch":: nil                                                                                                                                                                         ^                                                                                                                                                                                                                                                                                              scala> val l3 = "Java" :: "sql" :: "DBMS" :: "Pytorch":: Nil                                                                                                                l3: List[String] = List(Java, sql, DBMS, Pytorch)                                                                                                                                                                                                                                                                                                       scala> l3.foreach(println)                                                                                                                                                  Java                                                                                                                                                                        sql                                                                                                                                                                         DBMS                                                                                                                                                                        Pytorch                                                                                                                                                                                                                                                                                                                                                 scala> val l4 = l4 :+ "Big dATA"                                                                                                                                            <console>:22: error: recursive value l4 needs type                                                                                                                                 val l4 = l4 :+ "Big dATA"                                                                                                                                                            ^                                                                                                                                                                                                                                                                                                                                       scala> val l4 = l3 :+ "Big dATA"                                                                                                                                            l4: List[String] = List(Java, sql, DBMS, Pytorch, Big dATA)                                                                                                                                                                                                                                                                                             scala> l4                                                                                                                                                                   res7: List[String] = List(Java, sql, DBMS, Pytorch, Big dATA)                                                                                                                                                                                                                                                                                           scala> l4 += "Spring"                                                                                                                                                       <console>:24: error: value += is not a member of List[String]                                                                                                                 Expression does not convert to assignment because receiver is not assignable.                                                                                                    l4 += "Spring"                                                                                                                                                                 ^                                                                                                                                                                                                                                                                                                                                             scala> l4 :+ "Spring"                                                                                                                                                       res9: List[String] = List(Java, sql, DBMS, Pytorch, Big dATA, Spring)                                                                                                                                                                                                                                                                                   scala> l4 +: "Hadoop"                                                                                                                                                       res10: scala.collection.immutable.IndexedSeq[Any] = Vector(List(Java, sql, DBMS, Pytorch, Big dATA), H, a, d, o, o, p)                                                                                                                                                                                                                                  scala> val l6 = "Hadoop" +: l4                                                                                                                                              l6: List[String] = List(Hadoop, Java, sql, DBMS, Pytorch, Big dATA)                                                                                                                                                                                                                                                                                     scala> l6.foreach(println)                                                                                                                                                  Hadoop                                                                                                                                                                      Java                                                                                                                                                                        sql                                                                                                                                                                         DBMS                                                                                                                                                                        Pytorch                                                                                                                                                                     Big dATA                                                                                                                                                                                                                                                                                                                                                scala> val list1 = "a"::"B"::"C"::"d"::"E"::Nil                                                                                                                             list1: List[String] = List(a, B, C, d, E)                                                                                                                                                                                                                                                                                                               scala> val list2 = list1 :+ "p"                                                                                                                                             list2: List[String] = List(a, B, C, d, E, p)                                                                                                                                                                                                                                                                                                            scala> list2.foreach(println)                                                                                                                                               a                                                                                                                                                                           B                                                                                                                                                                           C                                                                                                                                                                           d                                                                                                                                                                           E                                                                                                                                                                           p                                                                                                                                                                                                                                                                                                                                                       scala> list2.head                                                                                                                                                           res13: String = a                                                                                                                                                                                                                                                                                                                                       scala> list2.tail                                                                                                                                                           res14: List[String] = List(B, C, d, E, p)                                                                                                                                                                                                                                                                                                               scala> scala.collection.immutable.Vector                                                                                                                                    res15: collection.immutable.Vector.type = scala.collection.immutable.Vector$@660979aa                                                                                                                                                                                                                                                                   scala> l r1 = spark.sparkContext.parallelize(Seq(("A",100),("V",200),("C",300))                                                                                                  | ;                                                                                                                                                                    <console>:2: error: ')' expected but ';' found.                                                                                                                                    ;                                                                                                                                                                           ^                                                                                                                                                                                                                                                                                                                                                scala> vall r1 = spark.sparkContext.parallelize(Seq(("A",100),("V",200),("C",300))                                                                                               | ';                                                                                                                                                                   <console>:2: error: unclosed character literal                                                                                                                                     ';                                                                                                                                                                          ^                                                                                                                                                                                                                                                                                                                                                scala> val r1 = spark.sparkContext.parallelize(Seq(("A",100),("V",200),("C",300))                                                                                                | r1.collect                                                                                                                                                           <console>:2: error: ')' expected but '.' found.                                                                                                                                    r1.collect                                                                                                                                                                    ^                                                                                                                                                                                                                                                                                                                                              scala>        ^                                                                                                                                                             <console>:23: error: not found: value ^                                                                                                                                                   ^                                                                                                                                                                           ^                                                                                                                                                                                                                                                                                                                                         scala>                                                                                                                                                                                                                                                                                                                                                  scala> scala> val r1 = spark.sparkContext.parallelize(Seq(("A",100),("V",200),("C",300))                                                                                                                                                                                                                                                                // Detected repl transcript. Paste more, or ctrl-D to finish.                                                                                                                                                                                                                                                                                                | r1.collect                                                                                                                                                           <console>:2: error: ')' expected but '.' found.                                                                                                                                    r1.collect                                                                                                                                                                    ^                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   // Replaying 1 commands from transcript.                                                                                                                                                                                                                                                                                                                scala> val r1 = spark.sparkContext.parallelize(Seq(("A",100),("V",200),("C",300))                                                                                           r1.collect                                                                                                                                                                  <console>:2: error: ')' expected but '.' found.                                                                                                                                    r1.collect                                                                                                                                                                    ^                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          scala>                                                                                                                                                                                                                                                                                                                                                  scala> val r1 = spark.sparkContext.parallelize(Seq(("A", 100), ("V", 200), ("C", 300)))                                                                                     r1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[0] at parallelize at <console>:22                                                                                                                                                                                                                                                   scala> r1.collect                                                                                                                                                           res17: Array[(String, Int)] = Array((A,100), (V,200), (C,300))                                                                                                                                                                                                                                                                                          scala> val r2 = spark.sparkContext.textFile("C:/Users/hpvai/Downloads/scal.txt")                                                                                            r2: org.apache.spark.rdd.RDD[String] = C:/Users/hpvai/Downloads/scal.txt MapPartitionsRDD[2] at textFile at <console>:22                                                                                                                                                                                                                                scala>                                                                                                                                                                                                                                                                                                                                                  scala> r2.collect                                                                                                                                                           res18: Array[String] = Array(fxcuiujhgfsdyufdfty yfy yt hjt ytc tyf gyf gy  tz tdh ttyf fygy, "", "C:/Users/hpvai/Downloads/scal.txt")                                                                                                                                                                                                                  scala> val r3 =r2.flatMap(_.split(" "))                                                                                                                                     r3: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at flatMap at <console>:23                                                                                                                                                                                                                                                                   scala> r3.collect                                                                                                                                                           res19: Array[String] = Array(fxcuiujhgfsdyufdfty, yfy, yt, hjt, ytc, tyf, gyf, gy, "", tz, tdh, ttyf, fygy, "", "C:/Users/hpvai/Downloads/scal.txt", "")                                                                                                                                                                                                scala> val r2 = spark.sparkContext.textFile("C:/Users/hpvai/Downloads/scal.txt")                                                                                            r2: org.apache.spark.rdd.RDD[String] = C:/Users/hpvai/Downloads/scal.txt MapPartitionsRDD[5] at textFile at <console>:22                                                                                                                                                                                                                                scala> r2.collect                                                                                                                                                           res20: Array[String] = Array(Spark RDDs are a powerful way to handle large-scale data processing. They allow you to perform transformations like map, filter, and reduceByKey across distributed datasets efficiently. With their fault-tolerant design, RDDs can recover lost data automatically using line)                                                                                                                                                                                                                       scala> r2.collect                                                                                                                                                           res21: Array[String] = Array(Spark RDDs are a powerful way to handle large-scale data processing. They allow you to perform transformations like map, filter, and reduceByKey across distributed datasets efficiently. With their fault-tolerant design, RDDs can recover lost data automatically using line)    
                                                                                                                                                                                                                   scala> val r3 =r2.flatMap(_.split(" "))                                                                                                                                     r3: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[6] at flatMap at <console>:23                                                                                                                                                                                                                                                                   scala> r3.collect                                                                                                                                                           res22: Array[String] = Array(Spark, RDDs, are, a, powerful, way, to, handle, large-scale, data, processing., They, allow, you, to, perform, transformations, like, map,, filter,, and, reduceByKey, across, distributed, datasets, efficiently., With, their, fault-tolerant, design,, RDDs, can, recover, lost, data, automatically, using, line)                                                                                                                                                                                  scala> val r4 = spark.range(20).toDF.rdd                                                                                                                                    r4: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[12] at rdd at <console>:22                                                                                                                                                                                                                                                    scala> r4.collect                                                                                                                                                           res23: Array[org.apache.spark.sql.Row] = Array([0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19])                                                                                                                                                                                            scala> val r4 = spark.range(10).toDF().rdd                                                                                                                                  r4: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[18] at rdd at <console>:22                                                                                                                                                                                                                                                    scala> r4.collect                                                                                                                                                           res24: Array[org.apache.spark.sql.Row] = Array([0], [1], [2], [3], [4], [5], [6], [7], [8], [9])                                                                                                                                                                                                                                                        scala> val flat1 = sc.textFile(""C:/Users/hpvai/Downloads/scal.txt"                                                                                                         <console>:1: error: ')' expected but '.' found.                                                                                                                                    val flat1 = sc.textFile(""C:/Users/hpvai/Downloads/scal.txt"                                                                                                                                                                       ^                                                                                                             <console>:1: error: identifier expected but string literal found.                                                                                                                  val flat1 = sc.textFile(""C:/Users/hpvai/Downloads/scal.txt"                                                                                                                                                                        ^                                                                                                            <console>:1: error: unclosed string literal                                                                                                                                        val flat1 = sc.textFile(""C:/Users/hpvai/Downloads/scal.txt"                                                                                                                                                                            ^                                                                                                                                                                                                                                                                                    scala> val flat1 = sc.textFile("C:/Users/hpvai/Downloads/scal.txt")                                                                                                         flat1: org.apache.spark.rdd.RDD[String] = C:/Users/hpvai/Downloads/scal.txt MapPartitionsRDD[20] at textFile at <console>:23                                                                                                                                                                                                                            scala> val flat2 =flat1.map(r=> r.split(" ")                                                                                                                                     | )                                                                                                                                                                    flat2: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[21] at map at <console>:23                                                                                                                                                                                                                                                            scala> flat2.collect                                                                                                                                                        res25: Array[Array[String]] = Array(Array(Spark, RDDs, are, a, powerful, way, to, handle, large-scale, data, processing., They, allow, you, to, perform, transformations, like, map,, filter,, and, reduceByKey, across, distributed, datasets, efficiently., With, their, fault-tolerant, design,, RDDs, can, recover, lost, data, automatically, using, line))                                                                                                                                                                                                                                                                                                                                                scala> val rdd1 = sc.textFile("C:/Users/hpvai/Downloads/scal.txt")                                                                                                          rdd1: org.apache.spark.rdd.RDD[String] = C:/Users/hpvai/Downloads/scal.txt MapPartitionsRDD[23] at textFile at <console>:23                                                                                                                                                                                                                             scala> val res = rdd1.map(r => r.split(" ")                                                                                                                                      | )                                                                                                                                                                    res: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[24] at map at <console>:23                                                                                                                                                                                                                                                              scala> res.collect                                                                                                                                                          res26: Array[Array[String]] = Array(Array(122, 1333, 1, 3, 34, 6, 7, 7, 8, 4, 4, 4, 4, 4, 4444), Array(122, 1333, 1, 3, 34, 6, 7, 7, 8, 4, 4, 4, 4, 4, 4444), Array(122, 1333, 1, 3, 34, 6, 7, 7, 8, 4, 4, 4, 4, 4, 4444))                                                                                                                                                                                                                                                                                                          scala> val res = rdd1.flatmap(r => r.split(" ")                                                                                                                                  | )                                                                                                                                                                    <console>:23: error: value flatmap is not a member of org.apache.spark.rdd.RDD[String]                                                                                             val res = rdd1.flatmap(r => r.split(" ")                                                                                                                                                   ^                                                                                                                                                                                                                                                                                                                                 scala> val rdd1 = sc.textFile("C:/Users/hpvai/Downloads/scal.txt")                                                                                                          rdd1: org.apache.spark.rdd.RDD[String] = C:/Users/hpvai/Downloads/scal.txt MapPartitionsRDD[26] at textFile at <console>:23                                                                                                                                                                                                                             scala> val rdd2 = rdd1.flatMap(x => x.split(" "))                                                                                                                           rdd2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[27] at flatMap at <console>:23                                                                                                                                                                                                                                                                scala> rdd2.collect                                                                                                                                                         res27: Array[String] = Array(122, 1333, 1, 3, 34, 6, 7, 7, 8, 4, 4, 4, 4, 4, 4444, 122, 1333, 1, 3, 34, 6, 7, 7, 8, 4, 4, 4, 4, 4, 4444, 122, 1333, 1, 3, 34, 6, 7, 7, 8, 4, 4, 4, 4, 4, 4444)                                                                                                                                                                                                                                                                                                                                      scala> val rdd2 = rdd1.flatMap(x => x.split(" "))                                                                                                                           rdd2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[28] at flatMap at <console>:23                                                                                                                                                                                                                                                                scala> val xx = sc.parallelize(1 to 10)                                                                                                                                     xx: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[29] at parallelize at <console>:23                                                                                                                                                                                                                                                            scala> x.collect()                                                                                                                                                          <console>:23: error: not found: value x                                                                                                                                            x.collect()                                                                                                                                                                 ^                                                                                                                                                                                                                                                                                                                                                scala> xx.collect()                                                                                                                                                         res29: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)                                                                                                                                                                                                                                                                                                scala> val xx = sc.parallelize(Array(1,2,3,4))                                                                                                                              xx: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[30] at parallelize at <console>:23                                                                                                                                                                                                                                                            scala> val xy = sc.parallelize(Array(4,6,7,8))                                                                                                                              xy: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[31] at parallelize at <console>:23                                                                                                                                                                                                                                                            scala> val un = xx.union(xy)                                                                                                                                                un: org.apache.spark.rdd.RDD[Int] = UnionRDD[32] at union at <console>:24                                                                                                                                                                                                                                                                               scala> un.foreach(println)                                                                                                                                                  2                                                                                                                                                                           4                                                                                                                                                                           3                                                                                                                                                                           1                                                                                                                                                                           4                                                                                                                                                                           6                                                                                                                                                                           7                                                                                                                                                                           8                                                                                                                                                                                                                                                                                                                                                       scala>         
scala> val un = xx.intersection(xy)
un: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[38] at intersection at <console>:24

scala> un.foreach(println)
4

scala> val dup = sc.parallelize(Array(1,2,2,2,2,2,2,3,4,5,6))
dup: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[39] at parallelize at <console>:23

scala> val dis = dup.distinct()
dis: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[42] at distinct at <console>:23

scala> dis.foreach(println)
4
5
1
3
6
2





















------------------------------------------------------------------------------------------------------------------------------------------------------------
CHapter - 4
------------------------------------------------------------------------------------------------------------------------------------------------------------


      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.5.6
      /_/

Using Scala version 2.12.18 (Java HotSpot(TM) 64-Bit Server VM, Java 21.0.7)
Type in expressions to have them evaluated.
Type :help for more information.

scala> import spark.implicits._
import spark.implicits._
25/09/29 10:33:39 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors

scala> val col = Seq("Students","Marks")
col: Seq[String] = List(Students, Marks)

scala> val data = Seq(("Vaibhav",100),("Harsh",120),("Glen",123))
data: Seq[(String, Int)] = List((Vaibhav,100), (Harsh,120), (Glen,123))

scala> val rdd = sc.parallelize(data)
rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[0] at parallelize at <console>:27
                   ^

scala> rdd.collect
res1: Array[(String, Int)] = Array((Vaibhav,100), (Harsh,120), (Glen,123))

scala> val df = rdd.toDF()
df: org.apache.spark.sql.DataFrame = [_1: string, _2: int]

scala>

scala> df.show()
+-------+---+
|     _1| _2|
+-------+---+
|Vaibhav|100|
|  Harsh|120|
|   Glen|123|
+-------+---+


create a DF with 5 cols of employee name emp address emp dept
col -> represents the col names
_* -> expands sequence of col names into individual arguments

scala> val rdd = spark.sparkContext.parallelize(Seq((1,"John"),(2,"Alice"),(3,"Krish")))
rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[11] at parallelize at <console>:22

scala> val col = Seq("ID","Name")
col: Seq[String] = List(ID, Name)

scala> val df2 = spark.createDataFrame(rdd).toDF(col:_*)
df2: org.apache.spark.sql.DataFrame = [ID: int, Name: string]

scala> df2.show()
+---+-----+
| ID| Name|
+---+-----+
|  1| John|
|  2|Alice|
|  3|Krish|
+---+-----+


df using rdds students in which cols are stud reg no and names, address course marks insert 10 rows




scala> val df3 = spark.read
df3: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@1d252cf0

scala>   .option("header", "true")
res22: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@1d252cf0

scala> val df3 = spark.read.option("header",true).csv("C:/Users/hpvai/Downloads/sds.csv")
df3: org.apache.spark.sql.DataFrame = [Ind: string, Delhi: string ... 1 more field]

scala> df3.show
+----+------+----+
| Ind| Delhi| 332|
+----+------+----+
|Ind |Mumbai|5632|
+----+------+----+



scala> import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SparkSession

scala> val spk = SparkSession.builder().master("local[1]").appName("SparkExample").getOrCreate()
25/10/04 10:22:04 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
spk: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@19f31c85

scala> val df = spark.read.option("header",true).csv("C:/Users/hpvai/Downloads/sds.csv")
df: org.apache.spark.sql.DataFrame = [Name: string, City: string ... 3 more fields]

scala> df.printSchema()
root
 |-- Name: string (nullable = true)
 |-- City: string (nullable = true)
 |-- Rank: string (nullable = true)
 |-- Age: string (nullable = true)
 |-- Country: string (nullable = true)



scala> df.show()
+-------+------+----+---+-------+
|   Name|  City|Rank|Age|Country|
+-------+------+----+---+-------+
|   Glen|Mumbai|5632| 21|    Ind|
|William| Texas|5657| 23|     Us|
+-------+------+----+---+-------+

scala> df.createOrReplaceTempView("v1")


scala> spark.sql("select * from v1").show
+-------+------+----+---+-------+
|   Name|  City|Rank|Age|Country|
+-------+------+----+---+-------+
|   Glen|Mumbai|5632| 21|    Ind|
|William| Texas|5657| 23|     Us|
+-------+------+----+---+-------+



scala> import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SparkSession

scala> val df = spark.read.option("header",true).csv("C:/Users/hpvai/Downloads/sds.csv")
df: org.apache.spark.sql.DataFrame = [Name: string, City: string ... 3 more fields]

scala> df.printSchema()
root
 |-- Name: string (nullable = true)
 |-- City: string (nullable = true)
 |-- Rank: string (nullable = true)
 |-- Age: string (nullable = true)
 |-- Country: string (nullable = true)


scala> df.show()
+-------+---------+----+---+-------+
|   Name|     City|Rank|Age|Country|
+-------+---------+----+---+-------+
|   Glen|   Mumbai|5632| 21|    Ind|
|William|    Texas|5657| 23|     Us|
| Sophia|   London|5621| 24|     Uk|
| Rajesh|    Delhi|5789| 22|    Ind|
|  Emily|   Sydney|5900| 25|    Aus|
| Carlos|   Madrid|5821| 26|  Spain|
|    Mia| New York|5755| 23|     Us|
|  Akira|    Tokyo|6002| 27|    Jpn|
| Fatima|    Dubai|5733| 24|    Uae|
|   Chen|  Beijing|6100| 28|  China|
| Olivia|  Toronto|5881| 25|    Can|
|   Noah|  Chicago|5678| 22|     Us|
| Ananya|Bangalore|5802| 24|    Ind|
|  Lucas|S?o Paulo|5966| 26| Brazil|
|   Emma|    Paris|5850| 23| France|
+-------+---------+----+---+-------+


scala> df.createOrReplaceTempView("v2")

scala> spark.sql("select * from v2").show
+-------+---------+----+---+-------+
|   Name|     City|Rank|Age|Country|
+-------+---------+----+---+-------+
|   Glen|   Mumbai|5632| 21|    Ind|
|William|    Texas|5657| 23|     Us|
| Sophia|   London|5621| 24|     Uk|
| Rajesh|    Delhi|5789| 22|    Ind|
|  Emily|   Sydney|5900| 25|    Aus|
| Carlos|   Madrid|5821| 26|  Spain|
|    Mia| New York|5755| 23|     Us|
|  Akira|    Tokyo|6002| 27|    Jpn|
| Fatima|    Dubai|5733| 24|    Uae|
|   Chen|  Beijing|6100| 28|  China|
| Olivia|  Toronto|5881| 25|    Can|
|   Noah|  Chicago|5678| 22|     Us|
| Ananya|Bangalore|5802| 24|    Ind|
|  Lucas|S?o Paulo|5966| 26| Brazil|
|   Emma|    Paris|5850| 23| France|
+-------+---------+----+---+-------+


scala> spark.sql("select * from v2 where age in (21,22,23,24)").show
+-------+---------+----+---+-------+
|   Name|     City|Rank|Age|Country|
+-------+---------+----+---+-------+
|   Glen|   Mumbai|5632| 21|    Ind|
|William|    Texas|5657| 23|     Us|
| Sophia|   London|5621| 24|     Uk|
| Rajesh|    Delhi|5789| 22|    Ind|
|    Mia| New York|5755| 23|     Us|
| Fatima|    Dubai|5733| 24|    Uae|
|   Noah|  Chicago|5678| 22|     Us|
| Ananya|Bangalore|5802| 24|    Ind|
|   Emma|    Paris|5850| 23| France|
+-------+---------+----+---+-------+


scala> spark.sql("select * from v2 where country ='Ind'").show
+------+---------+----+---+-------+
|  Name|     City|Rank|Age|Country|
+------+---------+----+---+-------+
|  Glen|   Mumbai|5632| 21|    Ind|
|Rajesh|    Delhi|5789| 22|    Ind|
|Ananya|Bangalore|5802| 24|    Ind|
+------+---------+----+---+-------+



--------------------------------------------------------------------------------------------------------------
scala> val df = spark.read.option("header",true).csv("C:/Users/hpvai/Downloads/d1.csv")
df: org.apache.spark.sql.DataFrame = [Name: string, Age: string ... 2 more fields]

scala> df.printSchema
root
 |-- Name: string (nullable = true)
 |-- Age: string (nullable = true)
 |-- Department: string (nullable = true)
 |-- Salary: string (nullable = true)


scala> df.show
+-------+---+----------+------+
|   Name|Age|Department|Salary|
+-------+---+----------+------+
|  Alice| 28|        HR| 55000|
|    Bob| 34|        IT| 75000|
|Charlie| 25| Marketing| 50000|
|  Diana| 30|   Finance| 68000|
|  Ethan| 40|        IT| 90000|
|  Fiona| 29|        HR| 60000|
| George| 38|   Finance| 72000|
| Hannah| 27| Marketing| 52000|
|    Ian| 32|        IT| 80000|
|  Julia| 26|        HR| 58000|
+-------+---+----------+------+


scala> df.createOrReplaceTempView("vc")

scala> spark.sql("select avg(age) from vc")
res30: org.apache.spark.sql.DataFrame = [avg(age): double]

scala> spark.sql("select avg(age) from vc").show
+--------+
|avg(age)|
+--------+
|    30.9|
+--------+

scala> spark.sql("select name from vc where department = 'Finance'").show
+------+
|  name|
+------+
| Diana|
|George|
+------+
---------------------------------------------------------------------------------------------------------------





















